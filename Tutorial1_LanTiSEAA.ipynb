{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1: Usage of LanTiSEAA\n",
    "\n",
    "The class lantiseaa.LanTiSEAA is a classifier implementing the Language Time Series Enriched Authorship Attribution method and can be integrated with the sklearn framework (as an Estimator and a Predictor). LanTiSEAA can take customized time series transformers, feature extractor, baseline classifier, meta classifier and buffer object and perform classification on text data. The iterative framework for stacking time series feature groups described in the paper is not implemented in this class (for more details have a look at IteratieLanTiSEAA), instead, LanTiSEAA simply combines all features extracted from the time series methods and the predictions made by the baseline method (if given), select relevant features and make predictions using the meta classifier.\n",
    "\n",
    "Using a sample data set which contains 1% of the samples randomly selected from the Spooky Books Data Set described in the paper (while keeping the class distribution), this notebook shows an example of using LanTiSEAA to perform an authorship attribution classification task.\n",
    "\n",
    "# Table of Content\n",
    "1. [Import Data](#1)\n",
    "2. [Split train and test data set](#2)\n",
    "3. [Use LanTiSEAA to perform classification](#3) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Data <a class=\"anchor\" id=\"1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "sns.set()\n",
    "\n",
    "import lantiseaa\n",
    "import lantiseaa.ts as ts\n",
    "from lantiseaa import LanTiSEAA\n",
    "from lantiseaa.baseline import BOWMNB\n",
    "from lantiseaa.extractor import TsfreshTSFeatureExtractor\n",
    "from lantiseaa.buffer import LocalBuffer, MemoryBuffer\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LocalBuffer can be used as a project IO for reading and saving data. Here we use it to read the sample data set and save results produced during the execution. MemoryBuffer can also be used to save results (temporarily) on the computer memory, and it is the default buffer used for class LanTiSEAA. However, for research purposes, we want to save all the results on the disks and maybe look for usaful information later in the study. Hence, LocalBuffer is used in this tutorial.\n",
    "\n",
    "In default, LocalBuffer set the root project directory at one level beyound the module \"LanTiSEAA\" folder. E.g. if the folder \"LanTiSEAA\" is placed under a folder named \"my_project\", the \"my_project\" folder will be set as the project folder and all data will be read/saved from/to the \"my_project\" folder. We want to direct it to the place where we want it to be - in this case, the \"LanTiSEAA\" folder itself.\n",
    "\n",
    "Besides, in default, the subfolder which used to organize the data produced from this specific execution is not set. We want to specify a subfolder for better organization of the saved files, in this case - \"tutorial1\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "io = LocalBuffer(project_path=os.getcwd(), subfolder='tutorial1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "io.project_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "io.subfolder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use LocalBuffer to get absolute paths to files in the project folder (note that path for a file that does not exist can be returned):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "io.data(filename='sample_dataset.csv', subfolder='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "io.results(filename='somefile.hdf', subfolder='tutorial1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "io.figures(filename='somefig.png', subfolder='tutorial1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "io.classes(filename='someclass.pkl', subfolder='tutorial1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "io.project(filename='Tutorial1_LanTiSEAA.ipynb', folder='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use LocalMemory to read sample data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(io.data('sample_dataset.csv'))\n",
    "\n",
    "texts = dataset.text\n",
    "y = dataset.author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.shape)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's have a look at the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the authors in the data set\n",
    "y_values = y.value_counts()\n",
    "\n",
    "sns.barplot(y_values.index, y_values.values, alpha=0.8)\n",
    "plt.title('Distribution of authors in sample data set')\n",
    "plt.ylabel('Number of Occurrences', fontsize=12)\n",
    "plt.xlabel(\"Author\", fontsize=12)\n",
    "plt.show() # distribution is kept the same as the original data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Split train and test data set <a class=\"anchor\" id=\"2\"></a>\n",
    "Leave about 1/3 of the data as the test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_train, texts_test, y_train, y_test = train_test_split(texts, y, test_size=0.33, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_train = texts_train.reset_index(drop=True)\n",
    "texts_test = texts_test.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Use LanTiSEAA to perform classification <a class=\"anchor\" id=\"3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LanTiSEAA can take in customized ts_transformers, feature_extractor, baseline_classifier, meta_classifier and buffer. To implement customized ts_transformer, feature_extractor, baseline_classifier or buffer, look at the ts, extractor, baseline, and buffer modules and extend the base classes. Besides, either to use predict_proba function or predict function from the baseline_classifier to make predictions can be specified using the use_predict_proba parameter. \n",
    "\n",
    "In this notebook, we simply use the defaults, except a few changes. \n",
    "\n",
    "The buffer will be the LocalBuffer we used to access files on the local disk. \n",
    "\n",
    "While the default meta_classifier is sklearn's GradientBoostingClassifier, we will use XGBoost's XGBClassifier here as in the paper, because we already know the number of classes the XGBClassifier will be dealing with and hence we can instantiate the XGBClassifier. \n",
    "\n",
    "Besides, we want to change the fdr_level for the TsfreshTSFeatureExtractor as for such a small data set, fdr_level=0.001 used in the paper will filter out almost all features.\n",
    "\n",
    "In addition, we remove the TokenRankDistTransformer as it will take relatively longer time to extract time series features from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "meta_classifier = XGBClassifier(objective='multi:softprob', num_class=3, n_jobs=-1, nthread=-1)\n",
    "feature_extractor=TsfreshTSFeatureExtractor(fdr_level=15)\n",
    "clf = LanTiSEAA(ts_transformers=[ts.TokenLenSeqTransformer(), \n",
    "                                 ts.TokenFreqSeqTransformer(), \n",
    "                                 ts.TokenRankSeqTransformer(), \n",
    "                                 ts.TokenLenDistTransformer()], \n",
    "                feature_extractor=feature_extractor, meta_classifier=meta_classifier, buffer=io)\n",
    "clf.fit(texts_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf.relevant_features_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = clf.predict_proba(texts_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "io.read_records()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
